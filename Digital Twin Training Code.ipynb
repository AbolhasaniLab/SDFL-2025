{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00078abb",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea9ecee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## packages\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from sklearn.preprocessing import MinMaxScaler \n",
    "from sklearn.model_selection import train_test_split \n",
    "from scipy.stats import linregress\n",
    "from joblib import Parallel, delayed\n",
    "import psutil\n",
    "import re\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import TensorDataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fece6b7a",
   "metadata": {},
   "source": [
    "### ML Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce27ef27",
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate relative PLQY\n",
    "def rel_PLQY(y_abs, y_area):\n",
    "    # Convert y_abs and y_area to pytorch tensors if they are numpy arrays\n",
    "    if not torch.is_tensor(y_abs):\n",
    "        y_abs = torch.from_numpy(y_abs.astype(np.float32))\n",
    "    if not torch.is_tensor(y_area):\n",
    "        y_area = torch.from_numpy(y_area.astype(np.float32))\n",
    "    # data related to the dye and solvent\n",
    "    PLQY_ref = 0.1336\n",
    "    ABS_ref = 0.9115\n",
    "    area_ref = 198493.1521\n",
    "    # calculate relative_PLQY\n",
    "    PLQY_sample = PLQY_ref * (y_area / area_ref) * ((1 - torch.pow(torch.tensor(10), -ABS_ref)) / (1 - torch.pow(torch.tensor(10), -y_abs)))\n",
    "    return PLQY_sample\n",
    "## scale output\n",
    "def non_dim_y(y_initial):\n",
    "    # Convert y_initial to pytorch tensor if it isn't\n",
    "    if not torch.is_tensor(y_initial):\n",
    "        y_initial = torch.from_numpy(y_initial.astype(np.float32))\n",
    "    # Get min and max info\n",
    "    min_info, min_idx = torch.min(y_initial, axis=0, keepdim=True)\n",
    "    max_info, max_idx = torch.max(y_initial, axis=0, keepdim=True)\n",
    "    statNorm_info = torch.concatenate((min_info, max_info), axis=0)\n",
    "    # Nondimensionalize the y's\n",
    "    y_scaled = (y_initial - min_info) / (max_info - min_info)\n",
    "    return y_scaled, statNorm_info\n",
    "def dim_y(y_transform, statNorm_info):\n",
    "    # Get min and max info\n",
    "    min_info = torch.index_select(statNorm_info, dim=0, index=torch.tensor([0]))\n",
    "    max_info = torch.index_select(statNorm_info, dim=0, index=torch.tensor([1]))\n",
    "    # Give y's dimension\n",
    "    y_scaled = min_info + y_transform * (max_info - min_info)\n",
    "    return y_scaled\n",
    "## nondimensionalize inputs\n",
    "def non_dim_x(x_initial):\n",
    "    # For doping project\n",
    "    x_normalized = np.zeros((x_initial.shape[0], x_initial.shape[1] - 3))\n",
    "    x_normalized[:, 0] = (x_initial[:, 0] - 120) / (180 - 120) # x1 = temperature\n",
    "    x_normalized[:, 1] = (x_initial[:, 1] - 80) / (120 - 80) # x2 = CuI\n",
    "    x_normalized[:, 2] = (x_initial[:, 2] - 80) / (120 - 80) # x3 = ZnI2\n",
    "    x_normalized[:, 3] = (x_initial[:, 3] - 20) / (80 - 20) # x4 =  Cs-Oleate\n",
    "    x_normalized[:, 4] = (x_initial[:, 4] - 220) / (280 - 220) # x5 = ODE\n",
    "    return x_normalized\n",
    "## split dataset to training and validation\n",
    "def split_set(x, y, train_ratio = 0.8, seed_num = 4862): # important hyper-parameter\n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, train_size = train_ratio, random_state = seed_num)\n",
    "    return x_train, x_test, y_train, y_test\n",
    "## ENN structure\n",
    "def structure_info(num_models, layer_min = 8, layer_max = 10, node_min = 20, node_max = 40, seed_num = 100): # should tune these hyperparameters\n",
    "    random.seed(seed_num)\n",
    "    detail_all = []\n",
    "    for i in range (num_models):\n",
    "        num_layers = random.randint(layer_min, layer_max)\n",
    "        nodes_list = []\n",
    "        for j in range(num_layers):\n",
    "            num_nodes = random.randint(node_min, node_max)\n",
    "            nodes_list.append(num_nodes)\n",
    "        detail = [num_layers, nodes_list]\n",
    "        detail_all.append(detail)\n",
    "    return detail_all\n",
    "# Create Neural Network model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, structure, network_type, input_shape, output_shape):\n",
    "        super(NeuralNetwork, self).__init__() # Initialize the network object using the superclass constructor\n",
    "        self.layers = nn.ModuleList() #List of layers\n",
    "        self.network_type = network_type\n",
    "        input_size = input_shape[-1] # Last dimension stores num of input features\n",
    "        # If network is a feed-forward neural network\n",
    "        if self.network_type == \"ff\":\n",
    "            for num_layers, nodes_list in structure:\n",
    "                for num_nodes in nodes_list:\n",
    "                    self.layers.append(nn.Linear(input_size, num_nodes)) # Apply the linear layer\n",
    "                    self.layers.append(nn.ReLU()) # Apply the ReLU activation\n",
    "                    input_size = num_nodes #reset input size to be the number of nodes determined by structure_info function\n",
    "            self.layers.append(nn.Linear(input_size, output_shape[-1]))  # final layer outputs predicted y's\n",
    "        # If network is a cascade neural network\n",
    "        elif self.network_type == \"cascade\":\n",
    "            for num_layers, nodes_list in structure:\n",
    "                for num_nodes in nodes_list:\n",
    "                    self.layers.append(nn.Linear(input_size, num_nodes)) # Apply the linear layer\n",
    "                    self.layers.append(nn.ReLU()) # Apply the ReLU activation\n",
    "                    input_size += num_nodes #reset input size to be the number of nodes determined by structure_info function\n",
    "            self.output_layer = nn.Linear(input_size, output_shape[-1]) # number of output parameters\n",
    "    def forward(self, x):\n",
    "        # If network is a feed-forward neural network\n",
    "        if self.network_type == \"ff\": \n",
    "            for layer in self.layers:\n",
    "                x = layer(x)          \n",
    "            return x\n",
    "        # If network is a cascade neural network\n",
    "        elif self.network_type == \"cascade\":\n",
    "            outputs = x\n",
    "            for layer in self.layers:\n",
    "                if isinstance(layer, nn.Linear):\n",
    "                    x = layer(outputs)  # Apply the linear layer\n",
    "                elif isinstance(layer, nn.ReLU):\n",
    "                    x = layer(x)  # Apply the ReLU activation\n",
    "                    outputs = torch.cat((outputs, x), dim = outputs.ndim-1) # Concatenate the current output to all previous ReLU outputs, along the last dimension\n",
    "            x = self.output_layer(outputs)     \n",
    "            return x\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.patience_counter = 0\n",
    "        self.best_model_state = None\n",
    "        self.min_validation_loss = float('inf')\n",
    "    def should_stop_early(self, model, current_validation_loss):\n",
    "        # Reset counter if new minimum validation loss found\n",
    "        if current_validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = current_validation_loss\n",
    "            self.best_model_state = model.state_dict()\n",
    "            self.patience_counter = 0\n",
    "        # If validation loss is greater than minimum, increase the patience counter\n",
    "        elif current_validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.patience_counter += 1\n",
    "            if self.patience_counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    def get_best_model_state(self):\n",
    "        return self.best_model_state\n",
    "## Training the neural network using PyTorch\n",
    "def parallel_train_ML_pytorch(train_x, train_y, val_x, val_y, structure, network_type, batch_size=8, num_epochs=3000, learning_rate=1e-4, patience=500, min_delta=1e-4):\n",
    "    # Convert dataset to tensors if they are numpy arrays\n",
    "    if not torch.is_tensor(train_x):\n",
    "        train_x = torch.from_numpy(train_x.astype(np.float32))\n",
    "    if not torch.is_tensor(train_y):\n",
    "        train_y = torch.from_numpy(train_y.astype(np.float32))\n",
    "    if not torch.is_tensor(val_x):\n",
    "        val_x = torch.from_numpy(val_x.astype(np.float32))\n",
    "    if not torch.is_tensor(val_y):\n",
    "        val_y = torch.from_numpy(val_y.astype(np.float32))\n",
    "    # Instantiate training and test data\n",
    "    train_dataset = TensorDataset(train_x, train_y)\n",
    "    test_dataset = TensorDataset(val_x, val_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=len(val_y), shuffle=False)\n",
    "    # Create model, optimizer, early stopping class, and loss function \n",
    "    model = NeuralNetwork(structure=structure, network_type=network_type, input_shape=train_x.shape, output_shape=train_y.shape)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate) #lr - learning step\n",
    "    early_stopping = EarlyStopping(patience=patience, min_delta=min_delta)\n",
    "    criterion = nn.MSELoss()\n",
    "    # Data to store during training\n",
    "    history = {'epoch': [], 'train_loss': [], 'val_loss': []}\n",
    "    epochs = []\n",
    "    train_loss_list = []\n",
    "    val_loss_list = []\n",
    "    # Number of outputs to train\n",
    "    num_outputs = train_y.shape[-1]\n",
    "    # Train the NN\n",
    "    for epoch in range(num_epochs):\n",
    "        ## Model Training\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        epochs.append(epoch)\n",
    "        for inputs, labels in train_loader:\n",
    "            # Reset optimizer\n",
    "            optimizer.zero_grad()\n",
    "            # Feed inputs\n",
    "            outputs = model(inputs)\n",
    "            # Loss function is the sum of output losses\n",
    "            loss = None\n",
    "            for train_output_idx in range(num_outputs):\n",
    "                train_output = torch.index_select(outputs, dim=outputs.ndim-1, index=torch.tensor(train_output_idx))\n",
    "                train_label = torch.index_select(labels, dim=labels.ndim-1, index=torch.tensor(train_output_idx))\n",
    "                if loss is None:\n",
    "                    loss = criterion(train_output, train_label)\n",
    "                else:\n",
    "                    loss += criterion(train_output, train_label)\n",
    "            # Back propagation\n",
    "            loss.backward()\n",
    "            # Update optimizer\n",
    "            optimizer.step()\n",
    "            # Add loss to accumulated loss \n",
    "            running_loss += loss.item()\n",
    "        # Compute average loss per batch\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_loss_list.append(train_loss)\n",
    "        ## Model Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            # Validate the NN\n",
    "            for val_inputs, val_labels in test_loader:\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss = None\n",
    "                # Validation loss is the sum of output losses\n",
    "                for val_output_idx in range(num_outputs):\n",
    "                    val_output = torch.index_select(val_outputs, dim=val_outputs.ndim-1, index=torch.tensor(val_output_idx))\n",
    "                    val_label = torch.index_select(val_labels, dim=val_labels.ndim-1, index=torch.tensor(val_output_idx))\n",
    "                    if val_loss is None:\n",
    "                        val_loss = criterion(val_output, val_label).item()\n",
    "                    else:\n",
    "                        val_loss += criterion(val_output, val_label).item()\n",
    "            # Compute average loss per batch\n",
    "            val_loss /= len(test_loader)\n",
    "            val_loss_list.append(val_loss)\n",
    "        #print(f'Epoch [{epoch + 1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "        if early_stopping.should_stop_early(model, val_loss):\n",
    "            #print(f'Early stopping triggered at epoch {epoch + 1}')\n",
    "            break\n",
    "    # Load the best model\n",
    "    if early_stopping.get_best_model_state() is not None:\n",
    "        model.load_state_dict(early_stopping.get_best_model_state())\n",
    "    # Gather history\n",
    "    history['epoch'] = epochs\n",
    "    history['train_loss'] = train_loss_list\n",
    "    history['val_loss'] = val_loss_list\n",
    "    result = [model, history]\n",
    "    return result\n",
    "## deconvolute the model and training history\n",
    "def deconvolute_ML(ensemble_result):\n",
    "    ensemble_model = []\n",
    "    ensemble_history = []\n",
    "    for element in ensemble_result:\n",
    "        ensemble_model.append(element[0])\n",
    "        ensemble_history.append(element[1])\n",
    "    return ensemble_model, ensemble_history\n",
    "## plots related to the training process\n",
    "def learn_plot(history):\n",
    "    fig = plt.figure()\n",
    "    # loss values - MSE\n",
    "    plt.plot(history['train_loss'], color = 'blue')\n",
    "    plt.plot(history['val_loss'], color = 'red')\n",
    "    plt.title('Loss (MSE) - Epoch')\n",
    "    plt.ylabel('Loss (MSE)')\n",
    "    plt.xlabel('Epoch Number')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "## Plot for predicted vs. true outputs - for individual models\n",
    "def predict_plot_separate(model, x, y_true):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "    dim = y_true.shape[-1]\n",
    "    plt.figure(figsize = (3.5 * dim, 3.5), dpi = 200)\n",
    "    # Plot each output separately\n",
    "    for i in range(dim):\n",
    "        y_true_i = y_true[:,i]\n",
    "        model_prediction = model(x).detach().numpy()\n",
    "        y_predicted_i = model_prediction[:, i]\n",
    "        detail = linregress(y_true_i, y_predicted_i)\n",
    "        plt.subplot(1, dim, i + 1)\n",
    "        plt.scatter(y_true_i, y_predicted_i, color = 'blue', alpha = 0.8, label = 'y' + str(i) + ': $R^2 = $' + str(round((detail.rvalue ** 2), 4)))\n",
    "        plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "        # plt.title('y' + str(i + 1))\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.legend() \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "## plot related to the predicted vs. true outputs - for the entire ENN \n",
    "def predict_plot_mean(ensemble_NN, x, y_true):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "    y_pred = []\n",
    "    for model in ensemble_NN:\n",
    "        y_pred.append(model(x).detach().numpy())\n",
    "    y_pred_arr = np.concatenate(y_pred, axis = 1)\n",
    "    dim = y_true.shape[1]\n",
    "    y_pred_mean = []\n",
    "    y_pred_std = []\n",
    "    plt.figure(figsize = (3.5 * dim, 3.5), dpi = 200)\n",
    "    # Plot each output separately\n",
    "    for i in range(dim):\n",
    "        element = y_pred_arr[:, i::dim]\n",
    "        element_mean = np.mean(element, axis = 1)\n",
    "        element_std = np.std(element, axis = 1)\n",
    "        element_detail = linregress(y_true[:, i], element_mean)\n",
    "        y_pred_mean.append(element_mean[:, np.newaxis])\n",
    "        y_pred_std.append(element_std[:, np.newaxis])\n",
    "        plt.subplot(1, dim, i + 1)\n",
    "        plt.errorbar(y_true[:, i], element_mean, yerr = element_std, fmt = 'o', color = 'blue', alpha = 0.8, markersize = 5, label = 'y' + str(i) + ': $R^2 = $' + str(round((element_detail.rvalue ** 2), 4)))\n",
    "        plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "        # plt.title('y' + str(i + 1))\n",
    "        plt.ylabel('Predicted')\n",
    "        plt.xlabel('Actual')\n",
    "        plt.xlim([-0.05, 1.05])\n",
    "        plt.ylim([-0.05, 1.05])\n",
    "        plt.legend()\n",
    "    plt.tight_layout()\n",
    "    pred_mean = np.concatenate(y_pred_mean, axis = 1)\n",
    "    pred_std = np.concatenate(y_pred_std, axis = 1)\n",
    "    return pred_mean, pred_std\n",
    "# Predict relative PLQY and compare it to true relative PLQY\n",
    "def predict_plot_mean_PLQY(ensemble_NN, x, y_true, statNorm_info):\n",
    "    if not torch.is_tensor(x):\n",
    "        x = torch.from_numpy(x.astype(np.float32))\n",
    "    rel_PLQY_pred = []\n",
    "    for model in ensemble_NN:\n",
    "        predictions = model(x).detach()\n",
    "        y_pred_dim = dim_y(predictions, statNorm_info)\n",
    "        rel_PLQY_pred.append(rel_PLQY(y_pred_dim[:,[0]], y_pred_dim[:,[1]]).numpy())\n",
    "    rel_PLQY_pred_arr = np.concatenate(rel_PLQY_pred, axis = 1)\n",
    "    plt.figure(figsize = (3.5, 3.5), dpi = 200)\n",
    "    rel_PLQY_pred_mean = np.mean(rel_PLQY_pred_arr, axis = 1)\n",
    "    rel_PLQY_pred_std = np.std(rel_PLQY_pred_arr, axis = 1)\n",
    "    y_true_dim = dim_y(y_true, statNorm_info)\n",
    "    rel_PLQY_true = rel_PLQY(y_true_dim[:,0], y_true_dim[:,1]).numpy()\n",
    "    element_detail = linregress(rel_PLQY_true, rel_PLQY_pred_mean)\n",
    "    plt.errorbar(rel_PLQY_true, rel_PLQY_pred_mean, yerr = rel_PLQY_pred_std, fmt = 'o', color = 'blue', alpha = 0.8, markersize = 5, label = '$R^2 = $' + str(round((element_detail.rvalue ** 2), 4)))\n",
    "    plt.plot([0, 1],[0, 1], color = 'red', label = 'Parity')\n",
    "    plt.title('Relative PLQY')\n",
    "    plt.ylabel('Predicted')\n",
    "    plt.xlabel('True')\n",
    "    plt.xlim([0.0, 0.25])\n",
    "    plt.ylim([0.0, 0.25])\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    return rel_PLQY_pred_mean, rel_PLQY_pred_std\n",
    "## save the ENN that's been trained\n",
    "def save_ENN(ENN, path):\n",
    "    for n, model in enumerate(ENN):\n",
    "        model_path = os.path.join(path, str(n))\n",
    "        os.mkdir(model_path)\n",
    "        torch.save(model, os.path.join(model_path , \"model\" + str(n)))\n",
    "## load the ENN that's been saved\n",
    "def load_ENN(path):\n",
    "    model_idx = sorted(os.listdir(path), key = len)\n",
    "    ensemble = []\n",
    "    for idx in model_idx:\n",
    "        model_path = os.path.join(path, idx)\n",
    "        ensemble.append(torch.load(os.path.join(model_path, \"model\" + idx)))\n",
    "    return ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b692b26",
   "metadata": {},
   "source": [
    "### Important Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e079a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_type=\"cascade\"\n",
    "batch_size=16\n",
    "num_epochs=3000\n",
    "learning_rate=1e-3\n",
    "patience=100\n",
    "min_delta=1e-4\n",
    "num_models=20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56923bc1",
   "metadata": {},
   "source": [
    "### Dataset / Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1417291",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = input(\"Please enter the path for the directory that includes files: \")\n",
    "print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "FR = pd.read_csv(path + \"//\" + \"inputs.csv\", header = 0).to_numpy()\n",
    "y = pd.read_csv(path + \"//\" + \"outputs.csv\", header = 0).to_numpy()\n",
    "x_norm = non_dim_x(FR)\n",
    "y_norm, statNorm_info = non_dim_y(y)\n",
    "x_train, x_valid, y_train, y_valid = split_set(x_norm, y_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ed1b7f",
   "metadata": {},
   "source": [
    "### Train ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2053dcfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train the ENN\n",
    "start_ML = time()\n",
    "#     p = psutil.Process()\n",
    "#     p.nice(psutil.REALTIME_PRIORITY_CLASS)\n",
    "architecture = structure_info(num_models)\n",
    "result_aggregate_ML = Parallel(n_jobs = -2)(delayed(parallel_train_ML_pytorch)(x_train, y_train, x_valid, y_valid, [architecture[i]], network_type=network_type, batch_size=batch_size, num_epochs=num_epochs, learning_rate=learning_rate, patience=patience, min_delta=min_delta) for i in range(num_models))\n",
    "ensemble, history = deconvolute_ML(result_aggregate_ML)\n",
    "end_ML = time()\n",
    "print(\"------------------------------------------------------------------------------------------------------------\")\n",
    "print(f\"The execution time to train the ensemble NN is {str(end_ML - start_ML)} seconds.\")\n",
    "print(\"------------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce09be2f",
   "metadata": {},
   "source": [
    "### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6f1841",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for his in history:\n",
    "#     learn_plot(his)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb8203b",
   "metadata": {},
   "source": [
    "### Parity Plots for Individual Models in ENN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a23dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in ensemble:\n",
    "#     predict_plot_separate(model, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff59747a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model in ensemble:\n",
    "#     predict_plot_separate(model, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a78ac4",
   "metadata": {},
   "source": [
    "### Overall ENN Performance on Individual Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5949648f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_train, std_train = predict_plot_mean(ensemble, x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff834e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_valid, std_valid = predict_plot_mean(ensemble, x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ed5695",
   "metadata": {},
   "source": [
    "### Overall ENN Performance on PLQY Proxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2957da9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PLQY_pred_mean_train, PLQY_pred_std_train = predict_plot_mean_PLQY(ensemble, x_train, y_train, statNorm_info) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef36df83",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PLQY_pred_mean_valid, PLQY_pred_std_valid = predict_plot_mean_PLQY(ensemble, x_valid, y_valid, statNorm_info) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
